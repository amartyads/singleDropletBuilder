common:
  preRun: |
    export OMP_NUM_THREADS=1

    cd "<workDir>"

    set +e

  postRun: |
    rm -f AutoPas*

  errCatch: |
    var=$?
    if [ "$var" -ne 0 ];then
      rm -f AutoPas*
      exit $var
    fi

manager:

  slurm:
    header: |
      #!/bin/bash

      #SBATCH --time=<wallTime>   # walltime limit (HH:MM:SS)
      #SBATCH --nodes=<numNodes>   # number of nodes
      #SBATCH --ntasks-per-node=72
      #SBATCH --cpus-per-task=1   # processor core(s) per node
      #SBATCH --partition=<partition>    # partition
      #SBATCH --job-name=<jobName>
      #SBATCH --output="drop%j" # job standard output file (%j replaced by job id)
      #SBATCH --hint=nomultithread
    
    dependency: |-
      #SBATCH --dependency=afterok:

    dependencySep: ","

    footer: |-
      echo "Job ${SLURM_JOB_ID} named ${SLURM_JOB_NAME} running from ${SLURM_SUBMIT_DIR} done" >> ~/completed_jobs.txt

  pbs:
    header: |
      #!/bin/bash

      #PBS -N <jobName>
      #PBS -l select=<numNodes>:node_type=rome:mpiprocs=128:ompthreads=1
      #PBS -l walltime=<wallTime>
      #PBS -j oe
      #PBS -o output
    
    dependency: |-
      #PBS -W depend=afterok:

    dependencySep: ":"

    footer: ""

system:

  hsuper:
    modules: |
      set -e

      ml gcc
      ml intel-oneapi-mpi

    runComm: |
      srun <execPath> <configFile>
    
    exec: "sbatch job.sh"

  hawk:
    modules: |
      set -e

      ml gcc
      ml openmpi/5.0.5
    
    runComm: |
      mpirun -n <numProcs> <execPath> <configFile>
    
    exec: "qsub job.sh"
